{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7f6058-d0d1-44af-b43a-cb4b06df03d8",
   "metadata": {},
   "source": [
    "# Prompting Capabilities \n",
    "\n",
    "- Note, you can try any of these prompts outside of this classroom, and without coding, by going to the chat interface [Le Chat](https://chat.mistral.ai/chat).\n",
    "  - You can sign up with a free account.\n",
    "  - Signing up for an account is **not** required to complete this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa37d97-10e4-425d-b852-15bd043662b9",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv==1.0.1 (from -r ..\\requirements.txt (line 5))\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting mistralai==0.1.6 (from -r ..\\requirements.txt (line 6))\n",
      "  Downloading mistralai-0.1.6-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting pandas==2.2.1 (from -r ..\\requirements.txt (line 7))\n",
      "  Downloading pandas-2.2.1-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting faiss-cpu==1.8.0 (from -r ..\\requirements.txt (line 8))\n",
      "  Downloading faiss_cpu-1.8.0-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting langchain==0.1.12 (from -r ..\\requirements.txt (line 9))\n",
      "  Downloading langchain-0.1.12-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langchain-mistralai==0.0.5 (from -r ..\\requirements.txt (line 10))\n",
      "  Downloading langchain_mistralai-0.0.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting llama-index==0.10.19 (from -r ..\\requirements.txt (line 11))\n",
      "  Downloading llama_index-0.10.19-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting llama-index-embeddings-mistralai==0.1.4 (from -r ..\\requirements.txt (line 12))\n",
      "  Downloading llama_index_embeddings_mistralai-0.1.4-py3-none-any.whl.metadata (645 bytes)\n",
      "Collecting llama-index-llms-mistralai==0.1.6 (from -r ..\\requirements.txt (line 13))\n",
      "  Downloading llama_index_llms_mistralai-0.1.6-py3-none-any.whl.metadata (677 bytes)\n",
      "Collecting mistral-haystack==0.0.1 (from -r ..\\requirements.txt (line 14))\n",
      "  Downloading mistral_haystack-0.0.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting panel==1.4.0 (from -r ..\\requirements.txt (line 15))\n",
      "  Downloading panel-1.4.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting httpx<0.26.0,>=0.25.2 (from mistralai==0.1.6->-r ..\\requirements.txt (line 6))\n",
      "  Downloading httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.10 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from mistralai==0.1.6->-r ..\\requirements.txt (line 6)) (3.10.13)\n",
      "Collecting pyarrow<16.0.0,>=15.0.0 (from mistralai==0.1.6->-r ..\\requirements.txt (line 6))\n",
      "  Downloading pyarrow-15.0.2-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from mistralai==0.1.6->-r ..\\requirements.txt (line 6)) (2.10.4)\n",
      "Collecting numpy<2,>=1.26.0 (from pandas==2.2.1->-r ..\\requirements.txt (line 7))\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from pandas==2.2.1->-r ..\\requirements.txt (line 7)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from pandas==2.2.1->-r ..\\requirements.txt (line 7)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from pandas==2.2.1->-r ..\\requirements.txt (line 7)) (2024.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain==0.1.12->-r ..\\requirements.txt (line 9)) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain==0.1.12->-r ..\\requirements.txt (line 9)) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain==0.1.12->-r ..\\requirements.txt (line 9)) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain==0.1.12->-r ..\\requirements.txt (line 9)) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain==0.1.12->-r ..\\requirements.txt (line 9)) (1.33)\n",
      "Collecting langchain-community<0.1,>=0.0.28 (from langchain==0.1.12->-r ..\\requirements.txt (line 9))\n",
      "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.31 (from langchain==0.1.12->-r ..\\requirements.txt (line 9))\n",
      "  Downloading langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.12->-r ..\\requirements.txt (line 9))\n",
      "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.1.12->-r ..\\requirements.txt (line 9))\n",
      "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain==0.1.12->-r ..\\requirements.txt (line 9)) (2.32.3)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.1.12->-r ..\\requirements.txt (line 9))\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tokenizers<0.16.0,>=0.15.1 (from langchain-mistralai==0.0.5->-r ..\\requirements.txt (line 10))\n",
      "  Downloading tokenizers-0.15.2-cp312-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting llama-index-agent-openai<0.2.0,>=0.1.4 (from llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading llama_index_agent_openai-0.1.7-py3-none-any.whl.metadata (644 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading llama_index_cli-0.1.13-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.19 (from llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading llama_index_core-0.10.68.post1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl.metadata (655 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.5 (from llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading llama_index_llms_openai-0.1.31-py3-none-any.whl.metadata (650 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading llama_index_program_openai-0.1.7-py3-none-any.whl.metadata (760 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading llama_index_readers_file-0.1.33-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting haystack-ai>=2.0.0b6 (from mistral-haystack==0.0.1->-r ..\\requirements.txt (line 14))\n",
      "  Downloading haystack_ai-2.9.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting bokeh<3.5.0,>=3.4.0 (from panel==1.4.0->-r ..\\requirements.txt (line 15))\n",
      "  Downloading bokeh-3.4.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting param<3.0,>=2.0.0 (from panel==1.4.0->-r ..\\requirements.txt (line 15))\n",
      "  Downloading param-2.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting pyviz-comms>=2.0.0 (from panel==1.4.0->-r ..\\requirements.txt (line 15))\n",
      "  Downloading pyviz_comms-3.0.4-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting xyzservices>=2021.09.1 (from panel==1.4.0->-r ..\\requirements.txt (line 15))\n",
      "  Downloading xyzservices-2024.9.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting markdown (from panel==1.4.0->-r ..\\requirements.txt (line 15))\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: markdown-it-py in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from panel==1.4.0->-r ..\\requirements.txt (line 15)) (3.0.0)\n",
      "Collecting linkify-it-py (from panel==1.4.0->-r ..\\requirements.txt (line 15))\n",
      "  Downloading linkify_it_py-2.0.3-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting mdit-py-plugins (from panel==1.4.0->-r ..\\requirements.txt (line 15))\n",
      "  Downloading mdit_py_plugins-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.48.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from panel==1.4.0->-r ..\\requirements.txt (line 15)) (4.67.1)\n",
      "Collecting bleach (from panel==1.4.0->-r ..\\requirements.txt (line 15))\n",
      "  Downloading bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from panel==1.4.0->-r ..\\requirements.txt (line 15)) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->-r ..\\requirements.txt (line 9)) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->-r ..\\requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->-r ..\\requirements.txt (line 9)) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->-r ..\\requirements.txt (line 9)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->-r ..\\requirements.txt (line 9)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->-r ..\\requirements.txt (line 9)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->-r ..\\requirements.txt (line 9)) (1.18.3)\n",
      "Collecting Jinja2>=2.9 (from bokeh<3.5.0,>=3.4.0->panel==1.4.0->-r ..\\requirements.txt (line 15))\n",
      "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting contourpy>=1.2 (from bokeh<3.5.0,>=3.4.0->panel==1.4.0->-r ..\\requirements.txt (line 15))\n",
      "  Using cached contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: packaging>=16.8 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from bokeh<3.5.0,>=3.4.0->panel==1.4.0->-r ..\\requirements.txt (line 15)) (24.2)\n",
      "Collecting pillow>=7.1.0 (from bokeh<3.5.0,>=3.4.0->panel==1.4.0->-r ..\\requirements.txt (line 15))\n",
      "  Downloading pillow-11.1.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from bokeh<3.5.0,>=3.4.0->panel==1.4.0->-r ..\\requirements.txt (line 15)) (6.4.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.12->-r ..\\requirements.txt (line 9)) (3.24.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.12->-r ..\\requirements.txt (line 9)) (0.9.0)\n",
      "Collecting haystack-experimental (from haystack-ai>=2.0.0b6->mistral-haystack==0.0.1->-r ..\\requirements.txt (line 14))\n",
      "  Downloading haystack_experimental-0.4.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting lazy-imports (from haystack-ai>=2.0.0b6->mistral-haystack==0.0.1->-r ..\\requirements.txt (line 14))\n",
      "  Downloading lazy_imports-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting more-itertools (from haystack-ai>=2.0.0b6->mistral-haystack==0.0.1->-r ..\\requirements.txt (line 14))\n",
      "  Downloading more_itertools-10.6.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting networkx (from haystack-ai>=2.0.0b6->mistral-haystack==0.0.1->-r ..\\requirements.txt (line 14))\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: openai>=1.56.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from haystack-ai>=2.0.0b6->mistral-haystack==0.0.1->-r ..\\requirements.txt (line 14)) (1.59.4)\n",
      "Collecting posthog (from haystack-ai>=2.0.0b6->mistral-haystack==0.0.1->-r ..\\requirements.txt (line 14))\n",
      "  Downloading posthog-3.8.3-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx<0.26.0,>=0.25.2->mistralai==0.1.6->-r ..\\requirements.txt (line 6)) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx<0.26.0,>=0.25.2->mistralai==0.1.6->-r ..\\requirements.txt (line 6)) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx<0.26.0,>=0.25.2->mistralai==0.1.6->-r ..\\requirements.txt (line 6)) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx<0.26.0,>=0.25.2->mistralai==0.1.6->-r ..\\requirements.txt (line 6)) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx<0.26.0,>=0.25.2->mistralai==0.1.6->-r ..\\requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpcore==1.*->httpx<0.26.0,>=0.25.2->mistralai==0.1.6->-r ..\\requirements.txt (line 6)) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.12->-r ..\\requirements.txt (line 9)) (3.0.0)\n",
      "Collecting packaging>=16.8 (from bokeh<3.5.0,>=3.4.0->panel==1.4.0->-r ..\\requirements.txt (line 15))\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.12->-r ..\\requirements.txt (line 9)) (1.0.0)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.19->llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.19->llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.11.0,>=0.10.19->llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index==0.10.19->-r ..\\requirements.txt (line 11)) (1.6.0)\n",
      "Collecting nltk!=3.9,>=3.8.1 (from llama-index-core<0.11.0,>=0.10.19->llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index==0.10.19->-r ..\\requirements.txt (line 11)) (0.8.0)\n",
      "Collecting wrapt (from llama-index-core<0.11.0,>=0.10.19->llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting llamaindex-py-client<0.2.0,>=0.1.19 (from llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2->llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading llamaindex_py_client-0.1.19-py3-none-any.whl.metadata (760 bytes)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.19->-r ..\\requirements.txt (line 11)) (4.12.3)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading llama_parse-0.5.19-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->mistralai==0.1.6->-r ..\\requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->mistralai==0.1.6->-r ..\\requirements.txt (line 6)) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.1->-r ..\\requirements.txt (line 7)) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests<3,>=2->langchain==0.1.12->-r ..\\requirements.txt (line 9)) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests<3,>=2->langchain==0.1.12->-r ..\\requirements.txt (line 9)) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.12->-r ..\\requirements.txt (line 9)) (3.1.1)\n",
      "Collecting huggingface_hub<1.0,>=0.16.4 (from tokenizers<0.16.0,>=0.15.1->langchain-mistralai==0.0.5->-r ..\\requirements.txt (line 10))\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from tqdm>=4.48.0->panel==1.4.0->-r ..\\requirements.txt (line 15)) (0.4.6)\n",
      "Collecting webencodings (from bleach->panel==1.4.0->-r ..\\requirements.txt (line 15))\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting uc-micro-py (from linkify-it-py->panel==1.4.0->-r ..\\requirements.txt (line 15))\n",
      "  Downloading uc_micro_py-1.0.3-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from markdown-it-py->panel==1.4.0->-r ..\\requirements.txt (line 15)) (0.1.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.19->-r ..\\requirements.txt (line 11)) (2.6)\n",
      "Collecting filelock (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.1->langchain-mistralai==0.0.5->-r ..\\requirements.txt (line 10))\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting MarkupSafe>=2.0 (from Jinja2>=2.9->bokeh<3.5.0,>=3.4.0->panel==1.4.0->-r ..\\requirements.txt (line 15))\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting click<9.0.0,>=8.1.7 (from llama-parse>=0.4.0->llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "INFO: pip is looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Downloading llama_parse-0.5.18-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading llama_parse-0.5.17-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading llama_parse-0.5.16-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading llama_parse-0.5.15-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading llama_parse-0.5.14-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.5.13-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.5.12-py3-none-any.whl.metadata (6.9 kB)\n",
      "INFO: pip is still looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading llama_parse-0.5.11-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.5.10-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.5.9-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading llama_parse-0.5.8-py3-none-any.whl.metadata (6.4 kB)\n",
      "  Downloading llama_parse-0.5.7-py3-none-any.whl.metadata (6.4 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading llama_parse-0.5.6-py3-none-any.whl.metadata (6.1 kB)\n",
      "  Downloading llama_parse-0.5.5-py3-none-any.whl.metadata (6.1 kB)\n",
      "  Downloading llama_parse-0.5.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "  Downloading llama_parse-0.5.3-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Downloading llama_parse-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Downloading llama_parse-0.5.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Downloading llama_parse-0.5.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "  Downloading llama_parse-0.4.9-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting joblib (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.19->llama-index==0.10.19->-r ..\\requirements.txt (line 11))\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.19->llama-index==0.10.19->-r ..\\requirements.txt (line 11)) (2024.11.6)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from openai>=1.56.1->haystack-ai>=2.0.0b6->mistral-haystack==0.0.1->-r ..\\requirements.txt (line 14)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from openai>=1.56.1->haystack-ai>=2.0.0b6->mistral-haystack==0.0.1->-r ..\\requirements.txt (line 14)) (0.8.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\langchain\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.12->-r ..\\requirements.txt (line 9)) (1.0.0)\n",
      "Collecting monotonic>=1.5 (from posthog->haystack-ai>=2.0.0b6->mistral-haystack==0.0.1->-r ..\\requirements.txt (line 14))\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog->haystack-ai>=2.0.0b6->mistral-haystack==0.0.1->-r ..\\requirements.txt (line 14))\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading mistralai-0.1.6-py3-none-any.whl (15 kB)\n",
      "Downloading pandas-2.2.1-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.5 MB 3.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/11.5 MB 8.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.3/11.5 MB 11.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.0/11.5 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 12.2 MB/s eta 0:00:00\n",
      "Downloading faiss_cpu-1.8.0-cp312-cp312-win_amd64.whl (14.5 MB)\n",
      "   ---------------------------------------- 0.0/14.5 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 6.3/14.5 MB 29.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.8/14.5 MB 30.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.5/14.5 MB 29.4 MB/s eta 0:00:00\n",
      "Downloading langchain-0.1.12-py3-none-any.whl (809 kB)\n",
      "   ---------------------------------------- 0.0/809.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 809.1/809.1 kB 17.6 MB/s eta 0:00:00\n",
      "Downloading langchain_mistralai-0.0.5-py3-none-any.whl (10 kB)\n",
      "Downloading llama_index-0.10.19-py3-none-any.whl (5.6 kB)\n",
      "Downloading llama_index_embeddings_mistralai-0.1.4-py3-none-any.whl (2.6 kB)\n",
      "Downloading llama_index_llms_mistralai-0.1.6-py3-none-any.whl (4.2 kB)\n",
      "Downloading mistral_haystack-0.0.1-py3-none-any.whl (11 kB)\n",
      "Downloading panel-1.4.0-py3-none-any.whl (24.6 MB)\n",
      "   ---------------------------------------- 0.0/24.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/24.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/24.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.1/24.6 MB 4.3 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.1/24.6 MB 4.3 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 3.1/24.6 MB 3.0 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 4.2/24.6 MB 3.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.2/24.6 MB 3.3 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.2/24.6 MB 3.1 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 6.3/24.6 MB 3.3 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 7.1/24.6 MB 3.3 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 7.3/24.6 MB 3.4 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 8.4/24.6 MB 3.3 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.4/24.6 MB 3.3 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 9.4/24.6 MB 3.4 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 9.4/24.6 MB 3.4 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 10.5/24.6 MB 3.2 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 11.5/24.6 MB 3.3 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 11.5/24.6 MB 3.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 12.6/24.6 MB 3.3 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 13.6/24.6 MB 3.3 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 14.7/24.6 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.7/24.6 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.7/24.6 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.7/24.6 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.8/24.6 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.8/24.6 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.8/24.6 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.8/24.6 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.8/24.6 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.8/24.6 MB 3.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.9/24.6 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.9/24.6 MB 3.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.0/24.6 MB 3.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.0/24.6 MB 3.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.0/24.6 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.6/24.6 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading bokeh-3.4.3-py3-none-any.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 6.3/7.0 MB 32.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 28.9 MB/s eta 0:00:00\n",
      "Downloading haystack_ai-2.9.0-py3-none-any.whl (419 kB)\n",
      "Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
      "Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 28.0 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.1.53-py3-none-any.whl (303 kB)\n",
      "Downloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
      "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Downloading llama_index_agent_openai-0.1.7-py3-none-any.whl (12 kB)\n",
      "Downloading llama_index_cli-0.1.13-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_core-0.10.68.post1-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 42.1 MB/s eta 0:00:00\n",
      "Downloading llama_index_embeddings_openai-0.1.11-py3-none-any.whl (6.3 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
      "Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 62.6 MB/s eta 0:00:00\n",
      "Downloading llama_index_llms_openai-0.1.31-py3-none-any.whl (12 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_program_openai-0.1.7-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.1.33-py3-none-any.whl (38 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Downloading param-2.2.0-py3-none-any.whl (119 kB)\n",
      "Downloading pyarrow-15.0.2-cp312-cp312-win_amd64.whl (25.3 MB)\n",
      "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 3.1/25.3 MB 62.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 4.2/25.3 MB 14.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 5.2/25.3 MB 10.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 6.3/25.3 MB 7.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 6.3/25.3 MB 7.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.3/25.3 MB 6.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.3/25.3 MB 6.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.3/25.3 MB 6.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.3/25.3 MB 6.8 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.4/25.3 MB 3.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.4/25.3 MB 4.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 11.5/25.3 MB 4.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.6/25.3 MB 4.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.6/25.3 MB 4.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.7/25.3 MB 4.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.7/25.3 MB 4.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 15.7/25.3 MB 4.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.8/25.3 MB 4.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.4/25.3 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 23.1/25.3 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.1/25.3 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.2/25.3 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.3/25.3 MB 5.4 MB/s eta 0:00:00\n",
      "Downloading pyviz_comms-3.0.4-py3-none-any.whl (83 kB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading tokenizers-0.15.2-cp312-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 30.9 MB/s eta 0:00:00\n",
      "Downloading xyzservices-2024.9.0-py3-none-any.whl (85 kB)\n",
      "Downloading bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Downloading linkify_it_py-2.0.3-py3-none-any.whl (19 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading mdit_py_plugins-0.4.2-py3-none-any.whl (55 kB)\n",
      "Using cached contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "Downloading Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading llama_parse-0.4.9-py3-none-any.whl (9.4 kB)\n",
      "Downloading llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading pillow-11.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.6/2.6 MB 25.2 MB/s eta 0:00:00\n",
      "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Downloading haystack_experimental-0.4.0-py3-none-any.whl (109 kB)\n",
      "Downloading lazy_imports-0.4.0-py3-none-any.whl (12 kB)\n",
      "Downloading more_itertools-10.6.0-py3-none-any.whl (63 kB)\n",
      "Downloading posthog-3.8.3-py2.py3-none-any.whl (64 kB)\n",
      "Downloading uc_micro_py-1.0.3-py3-none-any.whl (6.2 kB)\n",
      "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: webencodings, striprtf, monotonic, dirtyjson, xyzservices, wrapt, uc-micro-py, tenacity, python-dotenv, pypdf, pillow, param, packaging, numpy, networkx, more-itertools, MarkupSafe, markdown, lazy-imports, joblib, fsspec, filelock, click, bleach, backoff, pyviz-comms, pyarrow, posthog, pandas, nltk, mdit-py-plugins, linkify-it-py, Jinja2, huggingface_hub, httpx, faiss-cpu, deprecated, contourpy, tokenizers, mistralai, llamaindex-py-client, langsmith, bokeh, panel, llama-index-legacy, llama-index-core, langchain-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-llms-mistralai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-embeddings-mistralai, langchain-text-splitters, langchain-mistralai, langchain-community, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, langchain, llama-index-program-openai, llama-index-question-gen-openai, llama-index, haystack-experimental, haystack-ai, mistral-haystack\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "  Attempting uninstall: python-dotenv\n",
      "    Found existing installation: python-dotenv 0.21.0\n",
      "    Uninstalling python-dotenv-0.21.0:\n",
      "      Successfully uninstalled python-dotenv-0.21.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.2\n",
      "    Uninstalling packaging-24.2:\n",
      "      Successfully uninstalled packaging-24.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.1\n",
      "    Uninstalling numpy-2.2.1:\n",
      "      Successfully uninstalled numpy-2.2.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.2.10\n",
      "    Uninstalling langsmith-0.2.10:\n",
      "      Successfully uninstalled langsmith-0.2.10\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.29\n",
      "    Uninstalling langchain-core-0.3.29:\n",
      "      Successfully uninstalled langchain-core-0.3.29\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.5\n",
      "    Uninstalling langchain-text-splitters-0.3.5:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.5\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.3.14\n",
      "    Uninstalling langchain-community-0.3.14:\n",
      "      Successfully uninstalled langchain-community-0.3.14\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.14\n",
      "    Uninstalling langchain-0.3.14:\n",
      "      Successfully uninstalled langchain-0.3.14\n",
      "Successfully installed Jinja2-3.1.5 MarkupSafe-3.0.2 backoff-2.2.1 bleach-6.2.0 bokeh-3.4.3 click-8.1.8 contourpy-1.3.1 deprecated-1.2.15 dirtyjson-1.0.8 faiss-cpu-1.8.0 filelock-3.16.1 fsspec-2024.12.0 haystack-ai-2.9.0 haystack-experimental-0.4.0 httpx-0.25.2 huggingface_hub-0.27.1 joblib-1.4.2 langchain-0.1.12 langchain-community-0.0.38 langchain-core-0.1.53 langchain-mistralai-0.0.5 langchain-text-splitters-0.0.2 langsmith-0.1.147 lazy-imports-0.4.0 linkify-it-py-2.0.3 llama-index-0.10.19 llama-index-agent-openai-0.1.7 llama-index-cli-0.1.13 llama-index-core-0.10.68.post1 llama-index-embeddings-mistralai-0.1.4 llama-index-embeddings-openai-0.1.11 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48.post4 llama-index-llms-mistralai-0.1.6 llama-index-llms-openai-0.1.31 llama-index-multi-modal-llms-openai-0.1.9 llama-index-program-openai-0.1.7 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.33 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.9 llamaindex-py-client-0.1.19 markdown-3.7 mdit-py-plugins-0.4.2 mistral-haystack-0.0.1 mistralai-0.1.6 monotonic-1.6 more-itertools-10.6.0 networkx-3.4.2 nltk-3.9.1 numpy-1.26.4 packaging-23.2 pandas-2.2.1 panel-1.4.0 param-2.2.0 pillow-11.1.0 posthog-3.8.3 pyarrow-15.0.2 pypdf-4.3.1 python-dotenv-1.0.1 pyviz-comms-3.0.4 striprtf-0.0.26 tenacity-8.5.0 tokenizers-0.15.2 uc-micro-py-1.0.3 webencodings-0.5.1 wrapt-1.17.2 xyzservices-2024.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-experimental 0.3.4 requires langchain-community<0.4.0,>=0.3.0, but you have langchain-community 0.0.38 which is incompatible.\n",
      "langchain-experimental 0.3.4 requires langchain-core<0.4.0,>=0.3.28, but you have langchain-core 0.1.53 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r \"..\\requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3ac64-1295-4a0e-90ab-51c338c945f7",
   "metadata": {},
   "source": [
    "- Notice that it's \"mistralai\", and not \"mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "720663d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining the \"best\" French cheese can be quite subjective, as it depends on personal preferences, such as taste, texture, and aroma. France is renowned for its wide variety of cheeses, with estimates suggesting there are over 1,600 different types. Here are a few highly regarded French cheeses across various categories:\n",
      "\n",
      "1. **Soft Cheeses**:\n",
      "   - **Brie de Meaux**: Known for its creamy interior and slightly crumbly center.\n",
      "   - **Camembert de Normandie**: Famous for its rich, buttery flavor and soft, gooey interior.\n",
      "\n",
      "2. **Semi-Soft Cheeses**:\n",
      "   - **Reblochon**: A savory cheese from the Alps, often used in tartiflette.\n",
      "   - **Munster**: A strong-tasting cheese with a washed rind, popular in eastern France.\n",
      "\n",
      "3. **Hard Cheeses**:\n",
      "   - **Comté**: A nutty and complex cheese made from unpasteurized cow's milk.\n",
      "   - **Beaufort**: A firm, pale yellow cheese with a distinctive fruity and nutty flavor.\n",
      "\n",
      "4. **Blue Cheeses**:\n",
      "   - **Roquefort**: A tangy, crumbly blue cheese made from sheep's milk.\n",
      "   - **Bleu d'Auvergne**: A strong, creamy blue cheese with a pungent aroma.\n",
      "\n",
      "5. **Goat Cheeses**:\n",
      "   - **Chèvre**: A general term for goat cheese, which can vary from fresh and crumbly to aged and firm.\n",
      "   - **Crottin de Chavignol**: A small, cylindrical goat cheese with a nutty flavor.\n",
      "\n",
      "Ultimately, the best French cheese is the one that you enjoy the most. Experimenting with different types can be a delightful journey through the diverse world of French cheeses.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mistralai.client import MistralClient\n",
    "\n",
    "api_key = \"ePMByfktIUoCrdndNwwmbY1e8c9mzq8h\"\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "client = MistralClient(api_key=api_key)\n",
    "\n",
    "chat_response = client.chat(\n",
    "    model= model,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the best French cheese?\",\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff393a84-303a-48ae-97fe-3551f524f733",
   "metadata": {},
   "source": [
    "### Load API key and helper function\n",
    "- Note: You can view or download the helper.py file by clicking on the \"Jupyter\" logo to access the file directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6a5640-203a-4e7a-bc4c-4bfe78da4099",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from helper import load_mistral_api_key\n",
    "load_mistral_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e030d9c2-1ecb-4bf0-864d-9f96b41bd016",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I can help with a wide range of tasks and provide information on various topics. Here are some examples of what I can do:\n",
      "\n",
      "1. **Answer Questions**: I can provide information on a wide range of topics, from general knowledge to specific queries.\n",
      "2. **Explain Concepts**: I can help explain complex ideas in simpler terms.\n",
      "3. **Suggestions and Recommendations**: Whether it's books, movies, recipes, or travel destinations, I can offer suggestions based on your preferences.\n",
      "4. **Language Assistance**: I can help with translations, definitions, and grammar.\n",
      "5. **Technical Support**: I can offer guidance on software, hardware, and troubleshooting common tech issues.\n",
      "6. **Creative Writing**: I can help with writing, including essays, stories, and even poetry.\n",
      "7. **Mathematical Help**: I can assist with solving math problems and explaining mathematical concepts.\n",
      "8. **Personal Development**: I can offer advice on topics like productivity, time management, and wellness.\n",
      "9. **Educational Support**: I can help with homework, studying, and understanding educational materials.\n",
      "10. **Entertainment**: I can tell jokes, provide trivia, and engage in fun conversations.\n"
     ]
    }
   ],
   "source": [
    "from helper import mistral\n",
    "\n",
    "print(mistral(\"hello, what can you do?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98a29ef-4764-40b7-aed8-0e5d0502f985",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e9d1c-ca45-4d19-882c-07e077ea19ad",
   "metadata": {
    "height": 642
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    You are a bank customer service bot. \n",
    "    Your task is to assess customer intent and categorize customer \n",
    "    inquiry after <<<>>> into one of the following predefined categories:\n",
    "    \n",
    "    card arrival\n",
    "    change pin\n",
    "    exchange rate\n",
    "    country support \n",
    "    cancel transfer\n",
    "    charge dispute\n",
    "    \n",
    "    If the text doesn't fit into any of the above categories, \n",
    "    classify it as:\n",
    "    customer service\n",
    "    \n",
    "    You will only respond with the predefined category. \n",
    "    Do not provide explanations or notes. \n",
    "    \n",
    "    ###\n",
    "    Here are some examples:\n",
    "    \n",
    "    Inquiry: How do I know if I will get my card, or if it is lost? I am concerned about the delivery process and would like to ensure that I will receive my card as expected. Could you please provide information about the tracking process for my card, or confirm if there are any indicators to identify if the card has been lost during delivery?\n",
    "    Category: card arrival\n",
    "    Inquiry: I am planning an international trip to Paris and would like to inquire about the current exchange rates for Euros as well as any associated fees for foreign transactions.\n",
    "    Category: exchange rate \n",
    "    Inquiry: What countries are getting support? I will be traveling and living abroad for an extended period of time, specifically in France and Germany, and would appreciate any information regarding compatibility and functionality in these regions.\n",
    "    Category: country support\n",
    "    Inquiry: Can I get help starting my computer? I am having difficulty starting my computer, and would appreciate your expertise in helping me troubleshoot the issue. \n",
    "    Category: customer service\n",
    "    ###\n",
    "    \n",
    "    <<<\n",
    "    Inquiry: {inquiry}\n",
    "    >>>\n",
    "    Category:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d367f-cc2c-4857-abd5-d1f6a545ebc0",
   "metadata": {},
   "source": [
    "#### Ask Mistral to check the spelling and grammar of your prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a2ecd-e542-4863-96dc-29786c003799",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "response = mistral(f\"Please correct the spelling and grammar of \\\n",
    "this prompt and return a text that is the same prompt,\\\n",
    "with the spelling and grammar fixed: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f0db5-ba51-43ca-a4c4-dc2b072291ba",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bbf68-a0bc-4e8a-bb6b-8ae20f3f2e2d",
   "metadata": {},
   "source": [
    "#### Try out the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4625fdc0-c6ef-4dcf-bbc0-d250a0ed277c",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "mistral(\n",
    "    response.format(\n",
    "        inquiry=\"I am inquiring about the availability of your cards in the EU\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4252e995-f3fc-4e62-abdb-3e367df55cbe",
   "metadata": {},
   "source": [
    "## Information Extraction with JSON Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741a613-5b1b-4fb1-9843-1c5737f36cd5",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "medical_notes = \"\"\"\n",
    "A 60-year-old male patient, Mr. Johnson, presented with symptoms\n",
    "of increased thirst, frequent urination, fatigue, and unexplained\n",
    "weight loss. Upon evaluation, he was diagnosed with diabetes,\n",
    "confirmed by elevated blood sugar levels. Mr. Johnson's weight\n",
    "is 210 lbs. He has been prescribed Metformin to be taken twice daily\n",
    "with meals. It was noted during the consultation that the patient is\n",
    "a current smoker. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2bd7a-48c9-4c0b-8a4a-049a43045805",
   "metadata": {
    "height": 472
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Extract information from the following medical notes:\n",
    "{medical_notes}\n",
    "\n",
    "Return json format with the following JSON schema: \n",
    "\n",
    "{{\n",
    "        \"age\": {{\n",
    "            \"type\": \"integer\"\n",
    "        }},\n",
    "        \"gender\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"male\", \"female\", \"other\"]\n",
    "        }},\n",
    "        \"diagnosis\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"migraine\", \"diabetes\", \"arthritis\", \"acne\"]\n",
    "        }},\n",
    "        \"weight\": {{\n",
    "            \"type\": \"integer\"\n",
    "        }},\n",
    "        \"smoking\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"yes\", \"no\"]\n",
    "        }}\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a0b58-aae5-45e2-8496-714b884f16b6",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "response = mistral(prompt, is_json=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a9cb95-bb08-4929-b16c-eb19877f3c01",
   "metadata": {},
   "source": [
    "## Personalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf048b4-3e33-4753-af97-25b73c51ee6a",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Dear mortgage lender, \n",
    "\n",
    "What's your 30-year fixed-rate APR, how is it compared to the 15-year \n",
    "fixed rate?\n",
    "\n",
    "Regards,\n",
    "Anna\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de7c1e-60c2-4f35-a51a-115b12d65bb6",
   "metadata": {
    "height": 404
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "\n",
    "You are a mortgage lender customer service bot, and your task is to \n",
    "create personalized email responses to address customer questions.\n",
    "Answer the customer's inquiry using the provided facts below. Ensure \n",
    "that your response is clear, concise, and directly addresses the \n",
    "customer's question. Address the customer in a friendly and \n",
    "professional manner. Sign the email with \"Lender Customer Support.\"   \n",
    "      \n",
    "# Facts\n",
    "30-year fixed-rate: interest rate 6.403%, APR 6.484%\n",
    "20-year fixed-rate: interest rate 6.329%, APR 6.429%\n",
    "15-year fixed-rate: interest rate 5.705%, APR 5.848%\n",
    "10-year fixed-rate: interest rate 5.500%, APR 5.720%\n",
    "7-year ARM: interest rate 7.011%, APR 7.660%\n",
    "5-year ARM: interest rate 6.880%, APR 7.754%\n",
    "3-year ARM: interest rate 6.125%, APR 7.204%\n",
    "30-year fixed-rate FHA: interest rate 5.527%, APR 6.316%\n",
    "30-year fixed-rate VA: interest rate 5.684%, APR 6.062%\n",
    "\n",
    "# Email\n",
    "{email}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf50774-0f91-4e0e-86c9-1525f6045ebb",
   "metadata": {
    "height": 47,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = mistral(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfaad2c-411e-4221-80f0-7acd21ba398c",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "- We'll use this [article](https://www.deeplearning.ai/the-batch/mistral-enhances-ai-landscape-in-europe-with-microsoft-partnership-and-new-language-models) from The Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbab492-1d18-4832-86a9-2fba645e0e52",
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "newsletter = \"\"\"\n",
    "European AI champion Mistral AI unveiled new large language models and formed an alliance with Microsoft. \n",
    "\n",
    "What’s new: Mistral AI introduced two closed models, Mistral Large and Mistral Small (joining Mistral Medium, which debuted quietly late last year). Microsoft invested $16.3 million in the French startup, and it agreed to distribute Mistral Large on its Azure platform and let Mistral AI use Azure computing infrastructure. Mistral AI makes the new models available to try for free here and to use on its La Plateforme and via custom deployments.\n",
    "\n",
    "Model specs: The new models’ parameter counts, architectures, and training methods are undisclosed. Like the earlier, open source Mistral 7B and Mixtral 8x7B, they can process 32,000 tokens of input context. \n",
    "\n",
    "Mistral Large achieved 81.2 percent on the MMLU benchmark, outperforming Anthropic’s Claude 2, Google’s Gemini Pro, and Meta’s Llama 2 70B, though falling short of GPT-4. Mistral Small, which is optimized for latency and cost, achieved 72.2 percent on MMLU.\n",
    "Both models are fluent in French, German, Spanish, and Italian. They’re trained for function calling and JSON-format output.\n",
    "Microsoft’s investment in Mistral AI is significant but tiny compared to its $13 billion stake in OpenAI and Google and Amazon’s investments in Anthropic, which amount to $2 billion and $4 billion respectively.\n",
    "Mistral AI and Microsoft will collaborate to train bespoke models for customers including European governments.\n",
    "Behind the news: Mistral AI was founded in early 2023 by engineers from Google and Meta. The French government has touted the company as a home-grown competitor to U.S.-based leaders like OpenAI. France’s representatives in the European Commission argued on Mistral’s behalf to loosen the European Union’s AI Act oversight on powerful AI models. \n",
    "\n",
    "Yes, but: Mistral AI’s partnership with Microsoft has divided European lawmakers and regulators. The European Commission, which already was investigating Microsoft’s agreement with OpenAI for potential breaches of antitrust law, plans to investigate the new partnership as well. Members of President Emmanuel Macron’s Renaissance party criticized the deal’s potential to give a U.S. company access to European users’ data. However, other French lawmakers support the relationship.\n",
    "\n",
    "Why it matters: The partnership between Mistral AI and Microsoft gives the startup crucial processing power for training large models and greater access to potential customers around the world. It gives the tech giant greater access to the European market. And it gives Azure customers access to a high-performance model that’s tailored to Europe’s unique regulatory environment.\n",
    "\n",
    "We’re thinking: Mistral AI has made impressive progress in a short time, especially relative to the resources at its disposal as a startup. Its partnership with a leading hyperscaler is a sign of the tremendous processing and distribution power that remains concentrated in the large, U.S.-headquartered cloud companies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaede63d-7392-4f1c-8a87-507ee31fe246",
   "metadata": {
    "height": 472
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a commentator. Your task is to write a report on a newsletter. \n",
    "When presented with the newsletter, come up with interesting questions to ask,\n",
    "and answer each question. \n",
    "Afterward, combine all the information and write a report in the markdown\n",
    "format. \n",
    "\n",
    "# Newsletter: \n",
    "{newsletter}\n",
    "\n",
    "# Instructions: \n",
    "## Summarize:\n",
    "In clear and concise language, summarize the key points and themes \n",
    "presented in the newsletter.\n",
    "\n",
    "## Interesting Questions: \n",
    "Generate three distinct and thought-provoking questions that can be \n",
    "asked about the content of the newsletter. For each question:\n",
    "- After \"Q: \", describe the problem \n",
    "- After \"A: \", provide a detailed explanation of the problem addressed \n",
    "in the question.\n",
    "- Enclose the ultimate answer in <>.\n",
    "\n",
    "## Write a analysis report\n",
    "Using the summary and the answers to the interesting questions, \n",
    "create a comprehensive report in Markdown format. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505b0a5-411b-4804-aaef-ccecfa3d07be",
   "metadata": {
    "height": 47,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = mistral(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6590c2-2558-45be-84cb-b1b99dbc3776",
   "metadata": {},
   "source": [
    "#### Try it out for yourself\n",
    "- Feel free to copy-paste text from another article in The Batch, or any other blog or news article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed88147-8d43-4207-b45b-06543371f913",
   "metadata": {},
   "source": [
    "## The Mistral Python client\n",
    "- Below is the helper function that you imported from helper.py and used earlier in this notebook.\n",
    "- For more details, check out the [Mistral AI API documentation](https://docs.mistral.ai/api/)\n",
    "- To get your own Mistral AI API key to use on your own, outside of this classroom, you can create an account and go to the [console](https://console.mistral.ai/) to subscribe and create an API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef7a7f-ebd6-49c3-8f91-5f5d284edf17",
   "metadata": {
    "height": 353
   },
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage   \n",
    "\n",
    "def mistral(user_message, \n",
    "            model=\"mistral-small-latest\",\n",
    "            is_json=False):\n",
    "    client = MistralClient(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
    "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
    "\n",
    "    if is_json:\n",
    "        chat_response = client.chat(\n",
    "            model=model, \n",
    "            messages=messages,\n",
    "            response_format={\"type\": \"json_object\"})\n",
    "    else:\n",
    "        chat_response = client.chat(\n",
    "            model=model, \n",
    "            messages=messages)\n",
    "        \n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd279d2-d4cd-4465-9d65-8143a16c4bca",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
